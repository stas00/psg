<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
  <link rel="stylesheet" href="psg.css" type="text/css">
  <LINK REL="SHORTCUT ICON" HREF="favicon.ico" type="image/x-icon"/>
  <META NAME="description" content="System Administrator Pocket Survival Guide -  A series of notes for Sys Admin"/>
  <META NAME="keyword" content="Sys Admin, System Administrator, Solaris, HP-UX, AIX, Linux, Note, Notes, Pocket, Survival, Guide, psg, data center, power, electrical, plug, LYS, LKS, LAPPLAPP, PSG101, sn50, tin6150"/>
  <META NAME="Robots" CONTENT="all"/>
  <META NAME="Author" CONTENT="Tin Ho"/>

  <title>Pocket Survival Guide - R</title>
</head>

<body> 
<div class="navheader">
<table summary="Navigation header" width="100%">
  <tbody>
    <tr>
      <th colspan="9" align="center">
	  
<A HREF="http://tin6150.github.io/psg/">Sys Admin Pocket Survival Guide - OS</A>

      </th>
    </tr>
    <tr>
      <td align="left">  <a accesskey="h" href="psg.html">Home</a>	</td>
      <td align="center"><a accesskey="s" href="python.html">python</a>	</td>
      <td align="center"><a accesskey="p" href="perl.html">Perl</a>	</td>
      <td align="center"><a accesskey="d" href="k8s.html">Kubernetes</a>                </td>
      <td align="center"><a accesskey="x" href="singularity.html">Singularity</a>       </td>  <!--TBA-->
      <td align="center"><a accesskey="r" href="coreos.html">CoreOS</a> </td>
      <td align="center"><a accesskey="w" href="aws.html">AWS</a>       </td>
      <td align="center"><a accesskey="b" href="bigdata.html">BigData</a>       </td>
      <td align="center"><a accesskey="v" href="vagrant.html">Vagrant</a>       </td>
      <td align="right"> <a accesskey="a" href="shellScript.txt">shellScript</a></td>
    </tr>
  </tbody>
</table>
<hr></div>

<div class="chapter" lang="en">
<div class="titlepage">
</div>
</div>


<div align="CENTER">
<A HREF="https://xkcd.com/539/"><IMG SRC="fig/xkcd_boyfriend.png"></A>
</div>



<H1>R</H1>

<A NAME="101"></A>
<H2>101</H2>
<PRE>
R       # interactive cli
Rstudio # gui

# run single command in shell
Rscript -e "print(system('hostname'))"   

# run a list (set?) of commands by placing them in {}.  p_load() won't run unless the library is loaded explicity.
Rscript --quiet --no-readline --slave -e '{ library(pacman); p_load( ggplot2, scales ) }'


Special symbols

^   power. 2^8 
$   separate table name and column name, eg nhanes$gender, opt$indir
~   tilde operator, 
-   minus, substraction.  eg a-b . Just-dont-use-it-in-var-name, as it is treated as substraction of each terms listed!  

log(100)           # Think ln() here!  By default, R log has base e!  Bunch of wackos! :P
log(100, base=10)  # this is log with base 10
log(100, 10)       # same as above

</PRE>

See also: 
<LI><A HREF="#tilde">tilde operator and formula</A> is one of the weird thing in R.
<LI><A HREF="#model_operators">model operators</A> (below)


<A NAME="EpiStat"></A>
<A NAME="epistat"></A>
<H1>EpiStat </H1>
Stat for Epidemiologist

<PRE>
library("EpiStat")


# not sure if these all are from this library, but what I am learning in epi1.

nhanes <- read.csv("ps7_nhanes.csv",header=TRUE)

# converting a numeric 1, 2 to categorical male, female, but creating a new col (sec_cat) in existing table (nhanes)
nhanes$sex_cat = ifelse(nhanes$sex==1, "Male", "Female") 

tab1x2 <- table(nhanes$sex) # cmd to create table ... by counting freq of values in col sex

table2x2 <- table(nhanes$dpq_bin, nhanes$vitd_def)  # create 2x2 epi table
addmargins(table2x2)   # addmargins create total (row and col) 


# express table "nhanes" entries as proportions
prop.table(tab2)    
# OR
proportions(tab2)

sum(tab2) # return a scalar containing total freq (ie count)  ## here think of count row by group

help(CS) # Cohort Study measuring Risk
CS( x,         "cases",   "exposure", full=FALSE)
CS( tableName, "outcome", "exposure", full=FALSE)
CS(nhanes,     "dpq_bin", "sex_cat",  full=FALSE) 

</PRE>

<!-- PH 142 Stat 2021.1223 ## -->
<!-- once course is over, may move this section to the bottom of the page ++ -->
<A NAME="PH142"></A>
<A NAME="ph142"></A>
<H2>PH142 - Stats for Public Health</H2>

Much of the R code below are from the 
<A HREF="https://ph142-ucb.github.io/sp20/course-schedule/">
PH 142 stat class by Mi-Suk Kang Dufour</A>

<H5>Lecture 1: Intro to R</H5>
<PRE>
# File menu, new, R Markdown doc = will generate a new doc with basic example in it
# kinit button in toolbar will render the doc for web view
# To speed up knit, one time run in console.
tinytex::install_tinytex()
# http://rmarkdown.rstudio.com = Rmd info
# http://socviz.co/gettingstarted.html = tutorial for Rmd and kinit

```{r code-chunk-desc, echo = FALSE}
print("R code chunk goes between ```{r}  ``` in Rmd")
```

```{r another-code-eg, eval = FALSE}
# can use F or FALSE, T or TRUE
# echo = F   # do not echo command into output by kinit
# eval = F   # do NOT evaluate/interpret as code to run, just do verbatim text output [also for output of kinit]

bad-var-name = 0  # dont use - in variable name, R treats it as math substraction!  use _
```

```{r}
# this one below is example of invoking autograder to check this checkpoint
. = ottr::check("tests/p1.R")
```


help("library")
?library
??histogram	# double ? for "topic" search

functions/packages introducted in this lecture:
- ggplot2, 
- filter() aka stats::filter()

</PRE>


Resources:
<BR>
<BR>

<LI>


Hadley Wickham and Garrett Grolemund have written a helpful book about R that is freely available here:  
<A HREF="https://r4ds.had.co.nz/"> https://r4ds.had.co.nz/ </A>
</LI>

<LI>
UCLA has a website with many helpful examples here:  
<A HREF="https://stats.idre.ucla.edu/r/"> https://stats.idre.ucla.edu/r/ </A>


</LI>

<BR><BR>

<!-- Lecture 2 : https://r.datahub.berkeley.edu/user/tin/rstudio/  -->
<H5>Lecture 2: Working with data in R</H5>

<PRE>
library(readr)                              # provides read_csv
lake_data = read_csv("mercury-lake.csv")		# read csv into an array (R is 1-indexed).

head(lake_data)		# show first few lines
lake_data %>% head	# same, using unix pipe style 
dim(lake_data)		# dimension as num of rows, cols
str(lake_data)		# structure, compact display of the kind of data in the array
names(lake_data)	# aka colnames(lake_data), show column heading name  
			# this eg has 6 cols:  lakes ph chlorophyll mercury number_fish age_data


library(dplyr) 		# PH142 use dplyr extensively

# rename column heading names, note first param is the array variable holding the data.
# new_array    = rename(old_array,  new_col_name = old_col_name)
lake_data_tidy = rename(lake_data,  name_of_lake = lakes       )
#                                               ^^^ don't use <- here     <!> 

# rename multiple cols at same time:
lake_data_tidy = rename(lake_data,                  # array_name
                           name_of_lake = lakes,    # new_name = old_name
                           ph_level = ph)


# using %>% pipe method.  think unix bash cli:
lake_data_tidy = lake_data  %>% rename( new_name = old_name )


# there are some nuances with pipe, if run like this, rename result NOT stored back into the array!
# because there were no assignment operation where the rename() line ran
new_lake_data_tidy = lake_data  
new_lake_data_tidy              %>% rename( new_name = old_name )
new_lake_data_tidy %>% head



# selecting (choosing columns) to keep, think SQL
# new_array  = select(old_array,  col1,  col2, col3       )   # first param for select() is source array/table
smaller_data = select(lake_data,  lakes, ph,   chlorophyll)
smaller_data %>% head(2)

# Negative select, ie drop named columns, note the minus sign before the column name
# new_array    = select(old_array,  - col_name_to_drop)
smaller_data_2 = select(lake_data,  - age_data        )


# drop multiple columns, using pipe operator syntax
smaller_data_3 = lake_data %>% select(- age_data, ph)
smaller_data_3 %>% head(2)

View(lake_data)			# interactive table to view array data, note it is uppercase V.  utils::View(df)

# arrange() = display data with sorting
lake_data %>% arrange(ph)		          # show data sorted by value in the ph column, ascending order is default
lake_data %>% arrange(desc(ph))		    # show data sorted by value in the ph column, DESCending order
lake_data %>% arrange(age_data, - ph)	# can sort by mult cols, and ph in descending order


# mutate() perform operation on a column, adding it as new col to a new array
# new_array        = old_array %>% mutate(new_column_name     = old_col_name OPERATION )
lake_data_new_fish = lake_data %>% mutate(actual_fish_sampled = number_fish    * 100   )


# chain multiple operations together using pipe operator
tidy_lake_data = lake_data %>% 
  rename(lake_name = lakes) %>%
  mutate(actual_fish_sampled = number_fish * 100) %>%
  select(- age_data, - number_fish)

tidy_lake_data %>% head(3)



# filter() - this is like grep, keep record where col age_data has value "recent"
# new_array        = old_array %>% filter(col_name  FILTER_CONDITION             )
lake_data_filtered = lake_data %>% filter(age_data   ==     'recent'             )
lake_data_filtered = lake_data %>% filter(age_data   %in% c('recent','str2')     )
# %in% with a concat set of strings allow multiple == conditions to be specified.  eg match one of the listed string in the c() set.
# use != for not equal


# filter() can also with math operations
# eg: only keep records where math operation is satisfied (in col named ph, keep rows whose values s larger than 7)
lake_data_filtered = lake_data %>% filter(ph > 7)	
lake_data_filtered = lake_data %>% filter(age_data=`recent`)

# use %in% operator
# eg, filter (ie show) rows where lake (name) is Alligator or  Blue Cypress:
#                               VVV--- c() is the concatenate function in R to create a list.
lake_data %>% filter(lakes %in%  c("Alligator", "Blue Cypress")  ) 


# multiple filters at once, comma or & separated:
lake_data %>% filter(ph > 6 , chlorophyll > 30)
lake_data %>% filter(ph > 6 & chlorophyll > 30)
#                          ^^^--- use of & is optional, comma also implied to be an AND operation
lake_data %>% filter(ph > 6 | chlorophyll > 30)
#                          ^^^--- vertical bar is an OR operation




# group_by() and summarize() can create new summary tables with aggregate data
# note fn name is summarize, summary() does a very different thing!
# mean() and sd() calculates the mean (average) and standard deviation of variables.

lake_data %>% 
  group_by(age_data) %>% 
  summarize(mean_ph = mean(ph))

# eg multiple summary operations
lake_data %>% 
  group_by(age_data) %>% 
  summarize(mean_ph               = mean(ph),
            standard_deviation_ph = sd(ph)     )

</PRE>
<BR>

Reference material: <BR>

- [15 min intro to dplyr](https://www.youtube.com/watch?v=aywFompr1F4)
<BR>
- [Data wrangling cheat sheet](https://www.rstudio.com/wp-content/uploads/2015/02/data-wrangling-cheatsheet.pdf)
<BR>
<BR>

<!-- completed lecture 2 Rmd on 2022.0108 -->


<H5>Lecture 3: Visualizing/plotting data</H5>
<PRE>
 - Visualization of categorical data: use `ggplot`'s `geom_bar()`
 - Visualization of continuous  data: use `ggplot`'s `geom_histogram()`


1. 'ggplot' to set up a canvas for graphics
2. `geom_bar(stat = "identity")` to make a bar chart when you specify the y variable
3. `geom_histogram()` to make a histogram for which ggplot needs to calculate the count
4. `fct_reorder(var1, var2)` to reorder a categorical variable (`var1`) by a numeric variable (`var2`)
    - from the `forcats` package
5. `geom_point()` to make a plot with dots  (scatter plots also use this)
6. `geom_line()` to make a plot with lines



id_data = read_csv("Ch01_ID-data.csv")
library(ggplot2)
ggplot(id_data, aes(x = disease, y = percent_cases)) 	+   # aes specify axis
  geom_bar(stat = "identity")   +                           # geom_bar specify plotting bar chart
  labs(y = "Percent", x = "")   +                           # label the axis
  theme_minimal(base_size = 15)                             # font size for labels



eg2: 
fct_reorder()                    # order disease by decreasing percentage -- library(forcats) ?
aes( fill=type )                 # aes in geom_bar: color bar graph according to disease type (one of the col in data table)
theme(legend.position = "top")   # add legends of color vs types at top of plot

id_data = id_data %>% 
  mutate(disease_ordered = fct_reorder(disease,             percent_cases, .desc = T))
#        ^^new_col_name^               ^^what to rearrange  ^^order by this field/col.

ggplot(id_data, aes(x = disease_ordered, y = percent_cases)) + 
  geom_bar(stat = "identity", aes(fill = type)) +
  labs(y = "Percent", x = "")      +
  theme_minimal(base_size = 15)    +
  theme(legend.position = "top")



eg3, geom_histogram()


- Histograms look a lot like bar charts, except that the bars touch because the 
underlying scale is continuous and the order of the bars matters
- In order to make a histogram, the underlying data needs to be **binned** into 
categories and the number or percent of data in each category becomes the height 
of each bar. 
- the **bins** devide the entire range of data into a series of intervals and counts
the number of observations in each interval
- the intervals must be consecutive and non-overlapping and are almost always chosen to be of equal size


geom_point() produces an x-y sequence of points (but not connected by line).  
kinda like a x-y scatter plot, but seems like 1 value per x entry.

geom_line() is like geom_point(), but draws a continuous line instead of a series of points.


### Lab 2

### df CS_data_raw is reassigned to itself!  
### so think of this as appending column to existing df.
### fct_relevel affects plot output (ordering of categories), but does NOT change the data in the source table (df)
CS_data_raw <- CS_data_raw %>% 
  mutate(Income_Group_order = forcats::fct_relevel(Income_Group, "Low income", 
                                              "Lower middle income", "Upper middle income", 
                                              "High income: nonOECD", "High income: OECD"))

### df CS_data_raw is reassigned to itself!
### so think of this as appending column to existing df.
### mutate end up adding new column,  named Income_Group_order
### new col created using forcats::fct_relevel()
### if one do:
str(CS_data_raw)
### it shows that the new column store "Factor" types data, albeit View() doesn't show any difference
### Income_Group_order: Factor w/ 5 levels "Low income","Lower middle income


Factor as variable type to store categorical data, 
so that it is sortable.
eg

CS_data %>% pull(Income_Group_order) %>% levels()

lab02 p6 eg:
ggplot(CS_data, aes(x=Income_Group)) + geom_bar()
  ### geom_bar() is the implicit y-axis,
  ### here didn't specify stats="identity", so it counted the x axis var (Income_Group)
  ### use x=Income_Group_order to use the Factor var.  


p14 = ggplot(CS_data, aes(x=GDP_2006)) + geom_histogram( binwidth=8000)
p14

p14 = p14 + facet_wrap( ~ Income_Group_order )  # facet_wrap create a series of panel to ggplot, one per each variable entry in the  "~ var_class" Income_Group_order   ### hw02

ggsave( "lab02_cs_plot.png", plot = last_plot())   # save plot to file

## hw02

### add a vertical line to indicate median.  lty=1 is solid line, 2 is dashed line, 3 is dotted.
geom_vline(aes(xintercept = median(GDP_2006)),  lty=2)


</PRE>

<H5>Lecture 4/HW02</H5>
<PRE>

### manually calculate (Min, Q1, Median, Q3, Max)
### (Note that R's calc method is very slightly different than textbook, unless add "type=2" as option to quantile() )
### and add them as yintercept line to a boxplot

Min = min(CS_data$CS_rate_100)
Q1 = quantile(CS_data$CS_rate_100, probs = 0.25)
Median = median(CS_data$CS_rate_100)
Q3 = quantile(CS_data$CS_rate_100, probs = 0.75)
Max = max(CS_data$CS_rate_100)

five_num_summary = CS_data %>% summarize( min = Min,
                                          Q1 = Q1,
                                          median = Median,
                                          Q3 = Q3,
                                          max = Max)
fns2 = five_num_summary

p14b =  ggplot(data = CS_data, aes(y = CS_rate_100)) +
          geom_boxplot(col = 'black', fill = 'sienna2') +
          theme_minimal() + 
          geom_hline(aes(yintercept = fns2$min), col = 'green') +
          geom_hline(aes(yintercept = fns2$Q1), col = 'green') +
          geom_hline(aes(yintercept = fns2$median), col = 'green') +
          geom_hline(aes(yintercept = fns2$Q3),     col = 'green') +
          geom_hline(aes(yintercept = fns2$max),    col = 'green') 



</PRE>


<!-- Lecture 6 slide 43 -->
<H5>Lecture 6</H5>
<PRE>

## read data from excel spreadsheet:
library(readxl)
spending_dat = read_xlsx("country-spending.xlsx", sheet = 2, range = "A7:D47" )


library(dplyr)

## back ticks must be used when variable name has space in them
## thus use rename() to change the variable name (column heading)
spending_dat = spending_dat %>%
rename(country_code = Country.code,
       health_expenditure = `Health expenditure per capita`, # back ticks
       GDP = `GDP per capita`) # back ticks must be used when variable name has space in them
# new_name =  old_col_name


tidy(data_table)		# fit data per tidyverse convention?

lin_model =  lm ( y ~ x ) 	# lm() is linear regression fit model, for y ~ x (x is predictor)
glance( lin_model )		# provide data like r^2, etc
cor( x, y ) 			# correlation 

# ? correlation vs R^2 ??    correlation is r, but calc often return r^2 (R-squared)?  ## slide 25


glance( lin_model ) %>% pull( r.squared )  	# this return the r^2 value.  # slide 25

# is pull() like awk, grabbing column?  ++ ??
# or does it return a 1x1 only?

geom_abline() add the fitting line.   it need intercept and slope)
geom_abline(intercept = c, slope = m)
geom_abline( -46.75, 0.1358)

geom_text_repel()	# repel text, ie, don't render them with overlap. 	# slide 46


# stuff about prediction in p28, review++


## scatter plot: geom_point()
mana_death1 = ggplot(mana_data, aes(x = powerboats, y = deaths)) +
  geom_point() +
  theme_minimal(base_size = 15)

## + line plot
tmp1 = ggplot(CS_data, aes( x=CS_rate_100, y=GDP_2006) ) + geom_point() +  geom_line() 
tmp2 = tmp1 + geom_smooth()  # add smooth line best curve fit

# next, colour the points by income_group
# note that the aes() is placed inside geom_point         -----------VVVVVVV
tmp6  = ggplot(CS_data_log, aes( y=log_CS, x=log_GDP) ) + geom_point(    aes(col=Income_Group) )   + geom_smooth() 
ugly6 = ggplot(CS_data_log, aes( y=log_CS, x=log_GDP) ) + geom_point() + aes(col=Income_Group)     + geom_smooth() 
# if aes of coloring by income_group is tackled onto the parent plot object, it would make geom_smooth() interrupted by Income_Group rather than span across these categories, as seen in ugly6 above




</PRE>

<!-- Lecture 7 (wk3)-->
<H5>Lecture 7</H5>
<PRE>

lecture topic summary:
1. geom_bar(aes(col = var), stat = "identity", position = "dodge")
2. geom_bar(aes(col = var), stat = "identity", position = "stack")
     fct_relevel() to reorder grouping w/in bar charts
3. Marginal vs conditional distributions
4. Simpson’s Paradox


# slide 14, some crazy multiple ~ operator thing

two_way_data <- tribble(~ smoking, ~ lung_cancer, ~ percent, ~number,
"smoker", "lung cancer", 4.8, 12,
"smoker", "no lung cancer", 95.2,238)



</PRE>



<!-- hmm... maybe merge conflict... ph486 vs x10 -->
<!-- Lecture 8 (wk3)-->
<H5>Lecture 8</H5>
<PRE>



# SRS = Simple Random Sample, slide 33, 38
set.seed(123)    # make future run repeatable
CS_100_1 = CS_data %>% smaple_n(100)        # pick n=100 rnd sample from data source
CS_100_1 = CS_data %>%  sample_frac(0.05)   # do 5% rather than static n size

# Proprtionate Stratified Sampling , slide 40
# group them , then sample, this provide stratum equal size proportions (eg for epi group randomized study)
CS_10percent_grouped <- CS_data %>%
  group_by(HOSP_BEDSIZE) %>%
  sample_frac(0.1)
dim(CS_10percent_grouped)
CS_10percent_grouped%>%group_by(HOSP_BEDSIZE) %>% tally()

</PRE>



<BR><BR>
<HR>
<BR>

<!-- merge pending from bombat , move up...-->
<H5>HW03</H5>

<PRE>
case_when() eg, also note use of ~ :

insure_data = insure_data %>%
  mutate(bmi_cat = case_when(bmi < 16 ~ "Underweight",
                             bmi >= 16 & bmi < 25 ~ "Normal",
                             bmi >= 25 & bmi < 30 ~ "Overweight",
                             bmi >= 30 ~ "Obese")
                  )
;


types of variables in R:


p8 <- 'ordinal'
# p8 <- 'nominal'
# p8 <- 'continuous'
# p8 <- 'discrete'

Note that integers are considered discrete!  thus num_of_child "int" is discrete
continuous is only for floats


p11 = ggplot(insure_subset, aes(x=age,y=charges)) + geom_point() +
  labs(title="age vs insurance charges for smoker with normal bmi")
p16 = p11 + geom_abline(intercept=b,slope=m)
p20 = p16 + geom_abline(intercept=new_b,slope=new_m,lty=2,col="red")
# lty=2 means use dashed line

insure_model = lm(charges ~ age, data=insure_subset)    





paused in line 387 ++FIXME++
</PRE>

<H1>R</H1>


<A NAME="container"></A>
<A NAME="singularity"></A>
<A NAME="docker"></A>
<H2>Jupyter Notebook with R kernel Container</H2>

<PRE>

singularity pull shub://tin6150/DIOS_demonstration
singularity exec DIOS_demonstration_latest.sif /opt/conda/bin/jupyter lab --allow-root

-or-

docker
podman run  -it --entrypoint=/opt/conda/bin/jupyter  tin6150/r4envids lab --allow-root


</PRE>

<A NAME="jupyter"></A>
<H3>Jupyter Notebook IR Kernel Setup</H3>

<PRE>
conda install -c conda-forge jupyterlab

IRkernel is R kernel for Jupyther Notebook.  Need to install as R package, then add kernel spec to Jupyter Notebook:

Rscript -e "install.packages('IRkernel')"

# run multiple commands in Rscript by placing them inside {}, cmd separated by ;
Rscript --quiet --no-readline --slave -e '{ print("Hello") ; print("hi") }'
# p_load requires library(pacman) to be run first, so, try like this:
Rscript --quiet --no-readline --slave -e '{library(pacman) ; p_load( ggplot2, scales ) ;  print("Hello") ; print("hi") }'

# system wide
IRkernel::installspec(user = FALSE)

# per user
Rscript --no-readline --slave -e "IRkernel::installspec(user = TRUE)"


/home/tin/anaconda3/bin/jupyter lab 
# expect to have browser launched with URL http://localhost:8888/lab
# then click icon to create new R notebook

</PRE>

<H2>Environment</H2>

<PRE>

setenv R_LIBS /path/to/user/R-libraries

# setting R_LIBS will change both where R looks for lib to load and when installing libs 

library( rJava );	# see if it can load the R library named rJava (case sensitive)


listing all installed libraries:
Rscript -e 'library()' 




sI <- sessionInfo()
sI


system('env')	# this is running the env command via the shell and display that info
system('id -a; hostname')

# in jupyter notebook, the system() command output to console 
# use combination of intern=TRUE and wrap with print() just to be sure
print(system( 'uptime', intern=TRUE ))  


library(data.table)
fread( "cat /proc/loadavg", check.names=TRUE ) ## run system command and read result into data frame


</PRE>

<H2>Setting dir for user package install</H2>

<PRE>

setting user's own library install dir.
for installing CRAN libraries...

You can have a R_LIBS under your home dir.  
If you setup R_LIBS to be a shared folder, then your student can use the same library that you (or I, not as root) install.  



mkdir -p  ~/rlibs36
export R_LIBS=~/rlibs36    (or R_LIBS=/global/home/groups/ac_seasonal/rlibs36 )
I am assuming this for a new R 3.6, are libraries compatible with old version? For Perl I keep them in separate directories; python is a different beast)

# setting R_LIBS will change both where R looks for lib to load and when installing libs 

Running this as user should install a package called diagram:
    Rscript --quiet -e 'install.packages("diagram",    repos = "http://cran.us.r-project.org")'    

after install, you should see files added to ~/rlibs36

If the above don't work,
start interactive R as user
install.packages('diagram',lib='~/rlibs36')

this page has a good write up about this issue:
https://statistics.berkeley.edu/computing/R-packages

</PRE>




<H2>Installing packages</H2>

<PRE>

installing packages from cran:
(see r4eta or metabolics container)


scripted to run from shell:

Rscript --quiet -e 'install.packages("ggplot2",  repos = "http://cran.us.r-project.org")'    
Rscript --quiet -e 'install.packages(c("aCRM", "akima", "broom", "cluster", "clusterCrit"), repos = "http://cran.us.r-project.org")'

# do NOT have TAB(s) inside the arg list for packages(...), as Rscript can't tolerate it when invoked from a bash script.


Rscript --quiet --no-readline --slave ...
	# --quiet is same as --silent 
	# --slave was to make R run as quietly as possible 
	# --no-readline hopefully do less drawing on screen and make install log more readable...
	# there isn't a --no-verbose or --no-interactive


install many packages in single command:
https://stackoverflow.com/questions/29041423/how-to-install-multiple-packages

install.packages(c("EIAdata", "gdata", "ggmap", "ggplot2")) # rest omitted

have not tried yet:
install.r EIAdata gdata ggmap ggplot2    # rest omitted again

</PRE>

<!--
list from (not optimized)
cat ~/Downloads/initialization*.R | grep library | tr ";" "\n" | tr "()" ":" | grep library |  awk -F: '{print "\"" $2}' | sort -u  |  cat ~/Downloads/initialization*.R | grep library | tr ";" "\n" | tr "()" ":" | grep library |  awk -F: '{print "\"" $2 "\""}' | sort -u  | xargs | sed 's/ /\", \"/g'

-->

<H2>CSV example</H2>

Simple example reading csv file and getting summary info about it.
Utilizes magrittr from dplyr, which allow unix pipe-like operation using %&gt;%
<PRE>
library( 'magrittr')
print(getwd())
data <- read.csv("/mnt/CSV_adjoin/dacsjvnew_AVOC_07_All_Sp.csv")
data %>% head() # like unix cat data | head
# print(data)   # maybe big
data %>% summary()  # summary stat for each column of data, eg min,median,quartilers

str( data ) # see list of variables (column headers) 

</PRE>


<H2>Misc</H2>


<PRE>
capabilities() 		# see if support X11, etc
system("id;pwd")	# run OS commands

setwd("dirname")	# cd to dirname, most useful in interactive R session.
source("myFuncList.R")	# import function list from file

.libPaths()		# list path used to scan for R libs 

.libPaths(  c( ./libPaths(), '/home/tin/R/x86_64-pc-linux-gnu-library/3.4' ) )   # add path to library 



rm(list=ls())  		# remove all var in memory

</PRE>


<H2>Oracle connection</H2>

<PRE>
Sys.setenv(TNS_ADMIN="/site/conf/TNS_ADMIN",ORACLE_HOME="/usr/prog/oracle-client/11.1.0.6",ORACLE_SID='')
# The TNS_ADMIN may not matter if ORACLE_HOME has a network/admin with working conf for tns name resolution.
# ORACLE_SID is very important!  if set, the instance specifier in the dbConnect will be ignored, and db always connect to instance specified by ORACLE_SID !!

library( 'ROracle' )

ora=Oracle()
con=dbConnect(ora,user='user/pass@instance')
summary(con)
# verify connected to desired db)

result=dbGetQuery(con, "select * from tab")
# perform SQL query

result
# see result of query (display content of variable)


result2=dbListTables( con )
# see list of tables for the connected schema

</PRE>


<A NAME="gc"></A>
<H2>GC: Garbage Collection</H2>

<PRE>
gc(verbose=TRUE) 	# explicitly call garbage collection
ls() 			# list all vars
object.size( get("myData") )	
rm()
rm(list=ls())  		# remove all var in memory # probably useful in rstudio when playing, but not in actual program!
sort( sapply( ls(), function(x) { object.size(get(x))} ),decreasing=TRUE )	# list all vars and their sizes

vars are out of scope at the end of function, and gc can happen to them automatically

bigmemory leave data on file.  some packages are build using bigmemory

file-backed big matrix -- https://ljdursi.github.io/beyond-single-core-R/#/132 
data = read.big.matrix(...)

csv is slow.
HDF5 and netCDF recommended. 

</PRE>
ref: https://ljdursi.github.io/beyond-single-core-R/#/111

<H1>R Programming</H1>

<A NAME="101.2"></A>
<H5>101.2</H5>

<A HREF="https://storage.googleapis.com/kaggle-forum-message-attachments/8467/Very%20Short%20Intro%20to%20R.pdf">R short primer (pdf)</A> (page 12)

<PRE>

• function(arglist){expr}: 
function definition: do expr with list of arguments arglist

• if(cond){expr1}else{expr2}: if-statement:
if cond is true, then expr1, else expr2

• for(var in vec) {expr}: 
for-loop: the counter var runs through the vector vec 
and does expr each run

• while(cond){expr}: while-loop: while cond is
true, do expr each run


• x[n]: the n-th element   # R index starts at 1
• x[m:n]: the m-th to n-th element
• x[c(k,m,n)]: specific elements
• x[x&gt;m &amp; x&lt;n]: elements between m and n

• x$n: element of list or data frame named n.  ie, x is a data frame, n is the column name

• x[["n"]]: idem		# [[ ]] in R ... 

• [i,j]: element at i-th row and jth column
• [i, ]: row i


• return: show variable on the screen (used in functions)


• as.numeric or as.character: change class to
number or character string

• strptime: change class from character to
date-time (POSIX)


?"%*%"  # get help on topic, quote it if it has "funny" symbols

%*% # matrix multiply
%/% # ?? partitioning data into partition for shipping to different nodes 

%:% # compose or nest foreach objects - https://ljdursi.github.io/beyond-single-core-R/#/84

$>% # like unix pipe, used a lot by dplyr.  can be chained.

%in%	# membership evaluation (may not need to be a true math set)

%%  # modulus, ie reminder.  eg 7%%3 is 1
2^4 and 2**4 both seems to be power, get 16.  2^4 notation maybe preferred.

</PRE>



<H5>R stuff - NA : Not available data</H5>

<PRE>

NA is the token to indicate a value that is not availabe.  Kinda like UNDEF.  
It is not a string, and it is not NULL.
It is spelled exactly as NA , both caps.

> foo=NA
> foo
[1] NA
> bar=na
Error: object 'na' not found

• is.na: test if variable is NA

R by default will not compute on dataset that contain NA.
If you don’t mind about the missing data and
want to compute the statistics anyway, you can
add the argument na.rm=TRUE 
(Should I remove the NAs? Yes!).

> max(j, na.rm=TRUE)
[1] 2


</PRE>

<H5>R stuff - stat stuff</H5>


<PRE>

• lm(v1∼v2): linear fit (regression line) between 
vector v1 on the y-axis and v2 on the x-axis

• nls(v1∼a+b*v2, start=ls(a=1,b=0)): nonlinear fit. 
Should contain equation with variables
(here v1 and v2 and parameters (here a and b)
with starting values

• coef: returns coefficients from a fit
• summary: returns all results from a fit

• sum: sum of a vector
• mean: mean of a vector
• sd: standard deviation of a vector


• aggregate(x,by=ls(y),FUN="mean"): split
data set x into subsets (defined by y) and computes means of the subsets.
Result: a new vector.

• approx: interpolate. Argument: vector with NAs. 
Result: vector without NAs.

</PRE>




<H5>stat functions</H5>

Ref: <BR>
- <A HREF="https://www.stat.umn.edu/geyer/old/5101/rlook.html#:~:text=dnorm%20is%20the%20R%20function,standard%20deviation%20of%20the%20distribution.">"UNM Stat 5101"</A>
<BR>

- <A HREF="http://eric.univ-lyon2.fr/~ricco/tanagra/fichiers/en_Tanagra_Calcul_P_Value.pdf">CDF and PPF in Excel, R and Python</A> (good graphical guide covering what the p, q, d, r functions cover.)
<BR>

- <A HREF="https://docs.scipy.org/doc/scipy/reference/stats.html">SciPy stats function</A>
<BR>

<PRE>


prefix: p, q, d, r ::

p for "probability", the cumulative distribution function (c.d.f.)
	F(x) = P(X <= x)
   	Answer What is  P(X < 27.4) when X has Normal distribution with mean 50 and standard deviation of 20 aka N(50,20).  N often styled as cursive cap N.
   	if variance (&sigma;) is given, use the relation s.d. = sqrt(sigma) 
   	pnorm( 27.4, mean=50, sd=20, lower.tail=TRUE )
	pnorm( 27.4, 50, 20 )
   	In R, optional param default to mean=0, sd=1, lower.tail=TRUE.
   	Python: scipy.stats.norm.cdf( 27.4, loc=50, scale=20 )
   	Excel:  norm.dist( 27.4, 50, 20, TRUE ) 

	For calculation of the p-value in a right tailed test for a given z-score:
   	1 - pnorm( 27.4, 50, 20 )
   	pnorm(27.4, 50, 20, lower.tail=FALSE )  # lower.tail=FALSE -> upper/right tail
	Python: 
	1 - scipy.stats.norm.cdf( 27.4, 50, 20 )
	scipy.stats.norm.sf( 27.4, 50, 20 )     # sf = 1 - cdf


q for "quantile", the inverse c.d.f. - aka PPF(q) for probability ( 1 - &alpha; )
   	p = F(x)
	x = F-inv(p)
   	F-inv is styled as F^-1.
   	So given a number p between zero and one, qnorm looks up the p-th quantile of the normal distribution.
   	Answer What is F-inv(0.95) when X has the N(100, 15^2) distribution?
   	qnorm(0.95, 100, 15)
   	qnorm(0.95, mean=100, sd=15, lower.tail=TRUE)
   	qnorm(0.05, mean=100, sd=15, lower.tail=FALSE)   # from the relation (1-alpha) + alpha = 1, here alpha=0.95
   	Python: scipy.stats.norm.ppf( 0.95, loc=100, scale=15 )
   	Excel: norm.inv( 0.95, 100, 15 ) .  Excel has a norm.s.inv(0.05), but only take mean=0 sd=1.

   	
d for "density", the density function (p.f. or p.d.f.)
   		

r for "random", a random variable having the specified distribution
	Generating random numbers from a standard normal distribution
	N(miu=0,sigma=1)  # N cursive, miu=mean of 0, sigma=standard deviation of 1
	rnorm(n=1, mean=0, sd=1 )
	Python: scipy.stats.norm.rvs( loc=0, scale=1, size=1, random_state=123 ) # random_state is optional, use seed for repeatable/determinsistic result
	If n or size > 1 get a vector.
	Excel:  norm.s.inv( rand() )

pnorm,  qnorm,  dnorm,  and rnorm.  are the p-, q-, d-, r- fn for Normal Distribution

pbinom  qbinom  dbinom  rbinom - for binomial distribution
pchisq	qchisq	dchisq	rchisq - for Chi-Square   scipy.stats.chi2.cdf() etc
pexp	qexp	dexp	rexp   - for Exponential
plogis	qlogis	dlogis	rlogis - for Logistic
plnorm	qlnorm	dlnorm	rlnorm - for Log Normal
ppois	qpois	dpois	rpois  - for Poisson
pt	qt	dt	rt     - for Student t    scipy.stats.t.cdf() etc
punif	qunif	dunif	runif  - for Uniform

Note that for continuous distribution, some of these fn are defined using integrals, and R doesn't do integrations!


All function take option mean and standard deviation.  If omitted, default mean=0, sd=1.  

pnorm(27.4, mean=50, sd=20)
pnorm(27.4, 50, 20)



</PRE>

<A NAME="distributions"></A>
<BR>
<div align="CENTER">
<A HREF="https://loonylabs.org/2019/10/03/day45-365doa/">
<IMG SRC="https://loonylabs.files.wordpress.com/2019/10/distribution-park-joke.gif?w=590" ALT='The Statistics Theme Park "Ride the Distributions!'></A>
<BR>
Source: <A HREF="https://loonylabs.org/2019/10/03/day45-365doa/">Loony Labs - Day 45</A> or <A HREF="https://eigenblogger.com/2012/05/">Claudia @ Eigenblogger</A>
<BR>
</div>
<BR>
<!-- img cached as fig/claudia.eigenblogger.distribution-theme-park.gif, formerly as fig/loonylabs.distribution-park-joke.gif , but code load directly from source web site -->


<H5>assignment</H5>

<PRE>
a =  5
a <- 5

# both = and <- can be used for assignment
# But <- is prefered by the R crowd.  it also screw with my html code here, so maybe this R.html should go have been done in .rst ...

# there are some subtle differences, side effect, formula, etc.
# see <A HREF="https://renkun.me/2014/01/28/difference-between-assignment-operators-in-r/">https://renkun.me/2014/01/28/difference-between-assignment-operators-in-r/</A>
# overall, for better readability of R code, 
# it was suggested to only use <- for assignment and = for specifying named parameters.
# But I was not convinced and will likely continue to use = 
# at least in this R.html doc.

</PRE>

<PRE>
# converting a numeric 1, 2 to categorical male, female, but creating a new col (sec_cat) in existing table (nhanes)

nhanes$sex_cat = ifelse(nhanes$sex==1, "Male", "Female") 


string to integer conversion (search food: casting)
myNumber = strtoi(myString, base = 10)


</PRE>

<H4>Data Structures</H3>

<A HREF="http://adv-r.had.co.nz/Data-structures.html">Adv R</A>

<PRE>

1D:
vector: homegenoeus elements.  c()
list:   heterogeneous elements.  list()

2D:
matrix:     homegenoeus elements.  matrix()
dataframe:  heterogeneous elements, named columns. think table of data/spreadsheet :-D.  data.frame(), tidyverse

3D, multi-dimention:
array:     homogeneous elements.  1-indexed.  array(), dim()


attribute()
structure()

factor() to store categorical data -- ie predefined values

</PRE>




<H5>vector, matrix</H5>
<A HREF="https://storage.googleapis.com/kaggle-forum-message-attachments/8467/Very%20Short%20Intro%20to%20R.pdf">R short primer (pdf)</A>

<PRE>

vec1 = c( 1, 3, 5, 7)	# c = concatenate, paste together.  
vec2 = c( 2, 4, 6, 8)
vec3 = seq(10,25,5)     # aka: vec3 = seq(from=10,to=25,by=5)  # result: [1] 10 15 20 25

sum(vec2)


> mat1=matrix( data = c(1,2,3,4,5,6), ncol=3 )   # ncol define how many column the matrix has
> mat1
     [,1] [,2] [,3]
[1,]    1    3    5
[2,]    2    4    6

> mat[2,2]
[1] 4

matlb/octave matrix syntax is A LOT nicer :-D
pay special attention to the sequence of the numbers.

cbind	# column bind to create matrix
rbind 	# row    bind


</PRE>




<H5>array</H5>

R is 1-indexed, so first entry of df is 1  <BR>
A 1D array is a vector.  <BR>
A 2D array is a matrix.  <BR>
3D and higher dim structure is rarer in stats.  <BR>

<PRE>

dim()
length()
names()
rownames()
colnames()
dimnames()
t() - transpose

c() generalizes to cbind() and rbind() for matrix
abind() for arrays

as.matrix()
as.array()  -- for conversions

</PRE>


<H5>dataframe</H5>

R is 1-indexed, so first entry of df is 1

<PRE>

coord = data.frame(x = runif(n.total), y = runif(n.total), sampled = 0) 
coord.sampled = coord[coord$sampled == 1,]   # initial sites

data.frame (coord) is like a table
and $ separate the table name and the column name.
Thus, the above eg has 
coord$x
coord$y
coord$sampled
each of them is an array (representing the cell values across many rows of the table).
coord$x[1]  # table coord, column x, row 1  # R is 1-indexed.  (unlike python 0-indexed)
coord$x[2]  # table coord, column x, row 2


> t = data.frame(x = c(11,12,14), y = c(19,20,21), z = c(10,9,7))
> t
   x  y  z
1 11 19 10
2 12 20  9
3 14 21  7

> mean(t$x)
[1] 12.33333


typeof( t )
calss( t )
is.data.frame( t )

cbind(t, data.frame( z = 3:1 )
rbind(t, data.frame(...) )




</PRE>


It’s a common mistake to try and create a data frame by cbind()ing vectors together. This doesn’t work because cbind() will create a matrix unless one of the arguments is already a data frame. Instead use data.frame() directly
<BR>
Since a data frame is a list of vectors, it is possible for a data frame to have a column that is a list
<BR>


<H5>List</H5>

<PRE>
List in a simple form is a vector.
but it can be arbitrary number of object, each having different length.

syntax looks like it has some nested structure, pay close attention.

> L2=list(vec1,vec2)
> L2
[[1]]
[1] 1 3 5 7

[[2]]
[1] 2 4 6 8


> L = list(one=1, two=c(1,2),five=seq(1, 4, length=5))
> L
$one
[1] 1

$two
[1] 1 2

$five
[1] 1.00 1.75 2.50 3.25 4.00


[ ]   returns a list
[[ ]] returns the object which is stored in list
If it is a named list, then
    List$name or List[["name"]] will return same as List[[ ]]
While List["name"] returns a list

ref: https://stackoverflow.com/questions/32819539/proper-way-to-access-list-elements-in-r

for list returned by ans = mccollect( list(jobs...) )
typically ans[[1]], ans[[2]] etc will contain the original answer desired.

</PRE>


<H2>Strange R stuff</H2>

<A NAME="formula"></A>
<A NAME="tilde"></A>
<H3> Formula and tilde (~) operator</H3>

Coming from a programming world, R's definition of formula and its use of the tilde (~) operator is one of the weirest sh-IT of R!! =)

<BR>
<BR>

R has this concept of formula, and, at least from a user perspective, is different than function.
(Actually, a formula can be called 'f' and another function 'f' and they can coexist and be called correctly by the right context!)
<BR>
<BR>
For python programmer, forget about programming for a bit.  Put on the stat thinking cap :)  <BR>
Remember the flower classification example in data science class?
<A HREF="https://stackoverflow.com/questions/14976331/use-of-tilde-in-r-programming-language">stack overflow</A>
describes this construct: <BR>
<TT>Species ~ Sepal.Length + Sepal.Width + Petal.Length + Petal.Width</TT><BR>
as "Model Species depends on Sepal Length, Sepal Width, Petal Length and Petal Width"  <BR>
	<BR>

The tilde (~) operator	is often used for regression model.  <BR>
The LHS of the ~ operator is the name of the model (Species). <BR>
The RHS are the variables that is fed into the model, with the + operator being overloaded (don't literary means add them, but to treat each as "columns" to find covariance).  Most models use data frames, and each "variable" is a column vector of data.
<BR><BR>

<H3>formula has delayed evaluation</H3>

Ref: 
<A HREF="https://cran.r-project.org/web/packages/lazyeval/vignettes/lazyeval.html">Cran Lazy Eval</A>, scroll down to the Formulas section.  <BR><BR>


A formula capture delays the evaluation of an expression so that you can later evaluate it with f_eval(). ie
<PRE>
f <- ~ 1 + 2 + 3
</PRE>
The formula 'f' will be defined, but the RHS expression will not be calculated yet.
<BR>
Only when 
<PRE>
f_eval(f)
</PRE>
is executed is the expression calculated.

<BR>
Above was a one-sided formula.  <BR>
Below is a two-sided formula: <BR>

<PRE>
	g <- y ~ x + z
</PRE>

lazyeval provides abstraction like: <BR>
<PRE>
f_rhs(g)   # x + z
f_lhs(g)   # y         
</PRE>


<H3>Common formula</H3>

<A HREF="https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/formula">R Doc</A>
<BR><BR>

lm and glm are logistic regresion models (ie linear line of best fit).
<BR>
## seed_weight is "Y", the response aka predicted value
## seed_count  is "X", the exploratory ie input value to find prediction for
##lm( y ~ x )
seed_mod <- lm( seed_weight ~ seed_count, data = seed_data )
tidy( seed_mod ) 
seed_mod 
seed_mod %>% str         
# there are a lot more than just intercept and slope in the linear model, just not shown when just asked for it plainly.  tidy() is a decent compromise

##  linear_mode = lm( y ~ x , data="someTable")
m = linear_model$coefficients[2] # slope  (it is a "named number" in R)
b = linear_model$coefficients[1] # intercept

<!-- -->
<BR>
<BR>
the form y ~ model is interpreted as a specification that the response y is modelled by a linear predictor specified symbolically by model. Such a model consists of a series of terms separated by + operators. 
<BR>

<BR><BR>

<A ID="model_operators"></A>
Model operators:
<PRE>
+ 	separate a series of terms that affect the model.
: 	??
* 	factor crossing: a*b interpreted as a+b+a:b
^ 	crossing to specific degree.  eg (a+b)^2 for crossing twice
%in%   	terms on the left are  nested within those on the right.
-      	remove dependence on the specified series

I() - To avoid this confusion, the function I() can be used to bracket those portions of a model formula where the operators are used in their arithmetic sense


</PRE>


<OL>
<LI>	The lattice package uses them to <A HREF="https://www.rdocumentation.org/packages/lattice/topics/B_00_xyplot">specify the variables to plot.</A></LI>
<LI>	The ggplot2 package uses them to <A HREF="https://www.rdocumentation.org/packages/ggplot2/topics/facet_wrap">specify panels for plotting.</A></LI>
<LI>	The dplyr package uses them for <A HREF="https://cran.r-project.org/web/packages/lazyeval/vignettes/lazyeval.html">non-standard evaulation.</A></LI>


</OL>

<H3>Tilde operator</H3>


Try to understand these:

<PRE>
# sequences 
x <-   c(1:100)
y <- seq(0.1, 10, 0.1)
	
</PRE>

<PRE>
	
# all these 3 are the same:
plot( y     ~ I(x^3) ) 
plot( I(x^3), y      ) 
plot( x^3,    y      )    # no need to use I() function as ~ operator never used.

# note how ~ syntatically swapped the order of the independent and dependent variables x and y

# but this below is very different:
# ^ is interpreted as model operator for cross reference.  ( due to presence of ~ operator)
ploy( y ~ x^3 ) 

	
</PRE>
<!-- -->

<A HREF="https://stackoverflow.com/questions/8055508/in-r-formulas-why-do-i-have-to-use-the-i-function-on-power-terms-like-y-i/8055683#8055683">StackOverflow more extensive discussion on formula</A>
<BR>

<A HREF="http://blog.rtwilson.com/regression-in-python-using-r-style-formula-its-easy/">Regression in Python usinr R-style formula</A> seems to discuss use of ~ and regression formula in R.
<BR>

<PRE>
import pandas as pd
from statsmodels.formula.api import ols
	
model = ols("MPG ~ Year", data=df)
results = model.fit()
</PRE>


<A NAME="pipe"></A>
<H3>Pipe</H3>

<A HREF="https://stackoverflow.com/questions/24536154/what-does-mean-in-r">stack overflow</A>

<PRE>


%>% is defined by magrittr, which is used by dplyr.
It is the equivalent of unix shell pipe.

iris %>% head() %>% summary() is equiv to summary(head(iris))
which one to use is a matter of preference.

Note: there is also %in%

</PRE>
<!-- -->

<H2>apply family of function</H2>


see <A HREF="https://www.r-bloggers.com/2016/03/apply-lapply-rapply-sapply-functions-in-r/">R-bloggers</A>

<PRE>
apply(t(beaver1),1,max) ## apply function max to data, 1=row-wise)
lapply() returns a list
sapply() returns a vector instead of a list
tapply() does grouping before running summary function on them
by() ... think of GROUP_BY in SQL


</PRE>



<H1>Libraries</H1>

<H3>lifkit</H3>
tbd

<H2>ModeSim</H2>

<PRE>
geoR::lifkit

## likfit - Likelihood Based Parameter Estimation For Gaussian Random Fields
## https://www.rdocumentation.org/packages/geoR/versions/1.8-1/topics/likfit

SimAnneal()
kridge

~

raster::extract()


makCluster
registerDoParallel
%dopar%

</PRE>


<H2>Python using R</H2>

<OL>
<LI>Quick hack: Just write to file, in /dev/shm so it is RAM based. </LI>
<LI>https://towardsdatascience.com/from-r-vs-python-to-r-and-python-aa25db33ce17 </LI>
<LI>PypeR - R within Python</LI>
<LI>pyRserve - connect to a remote R server</LI>
<LI>rpy2 - runs embedded R in a Python process </LI>
<LI></LI>
</OL>

<H2>R using python</H2>

<OL>
<LI>rPython</LI>
<LI>SnakeCharmR - fork, more modern ver of rPython</LI>
<LI>PythonInR</LI>
<LI>reticulate</LI>
<LI></LI>
</OL>



<A NAME="Parallelization"></A>
<H1>Parallelization in R</H1>

2 categories of parallelization:
- Shared memory: ie, multiple cores, single machine memory space.  typically fork-based.
- distributed memory: multiple machines (multiple OS instances), using Socket, MPI, etc

6 main types/bundles of parallization exist in R:

<LI> same host, FORK based, parent/child memory model with copy-on-write: mclapply, mcparallel, mccollect.  <TT>multicore</TT> is easy if code already use <TT>lapply</TT>.  Not usable in Windows natively as it does not support fork().

<LI> Rdsm - shared memory within a single computer, so multiple PSOCK same-node processes can read/write to 1 single memory region.

<LI> PSOCK based multi-node parallelization, without needing MPI (cluster): SNOW: parLapply, clusterApplyLB 
<UL>
	<LI>Socket based, can work on windoze.
	<LI> Snow cluster is well suited for use in HPC env.   But better to use <EM><TT>parallel</TT></EM> rather than code using snow primitive, since it implent snow and multicore in a single library, as well as parallel random num generation.
</UL>


<LI> MPI based -- SNOW cluster implement on this as well.
<LI> pbdR, hiding some of the complexity of MPI
<LI> Hardoop/SparkR but that's a platforms of its own.
<LI> Packages that does parallelism under the hood.  eg BLAS, caret, many others 

<LI> future_map(), purrr -- https://byuistats.github.io/M335/parallel_furrr.html

<BR>


<BR><BR>

Tutorials <BR><BR>

Chris Paciorek (UC Berkeley Stats Dept)
<A HREF="https://htmlpreview.github.io/?https://github.com/berkeley-scf/tutorial-parallel-distributed/blob/master/parallel-dist.html">Parallel Processing for Distributed Computing</A> Cover R, Python, Matlab and C, as well as how to use them in a Slurm cluster.

<BR><BR>
Jonathan Dursi
<A HREF="https://ljdursi.github.io/beyond-single-core-R/#/1">Beyond Single-Core R</A> a slide deck on Parallelization in R.  
Summary slides: <BR>

<LI> Packages that does parallelism under the hood -- https://ljdursi.github.io/beyond-single-core-R/#/22
<LI> paralle/multicore with mc* -- https://ljdursi.github.io/beyond-single-core-R/#/51
<LI> cluster* parLapply snow -- https://ljdursi.github.io/beyond-single-core-R/#/73
<LI> foreach doParallel isplit %:% -- https://ljdursi.github.io/beyond-single-core-R/#/98
<LI> Best Practices -- https://ljdursi.github.io/beyond-single-core-R/#/106
<LI> bigmemory morder mpermute mwhich sub.big.matrix -- https://ljdursi.github.io/beyond-single-core-R/#/133
<LI> Rdsm -- shared memory within node by multiple process -- https://ljdursi.github.io/beyond-single-core-R/#/142

<BR>
<BR>


Dursi's github repo has data for running his example, and I wrapped that with a Jupyter Notebook container so everyone can try using this 
<A HREF="https://hub.docker.com/repository/docker/tin6150/beyond-single-core-r/general">Docker Container with Jupyter notebook server</A>:

<PRE>
Start jupyter notebook web server (on specific port, eg 6997):

docker run -p 6997:6997 -v "$PWD":/mnt -it --entrypoint=/opt/conda/bin/jupyter  tin6150/beyond-single-core-r  lab --allow-root  --no-browser --port=6997 --ip=0.0.0.0

Point web browser to something like
  http://127.0.0.1:6997/
and paste the token URL link shown on the terminal console

/mnt is a mount of the current dir (PWD) where you started the docker process, and files written there will persist after the container is terminated.  (Other files inside the container are ephemeral!)
</PRE>



<A NAME="mc"></A>
<H2>Parallelization in R using mc* functions </H2>

mclapply   <BR>
mcparallel  (likely just called parallel in early days)   <BR>
mccollect   <BR>
mc.core   <BR>

<BR>

Ref: 
<A HREF="https://ljdursi.github.io/beyond-single-core-R/#/26">Beyond single core R</A>
<BR>

<PRE>
mc*        are multi-core routines
mcparallel is good for parallel tasks
mclapply   is good for data parallelism - easily replace lapply()
mc.cores   really should represent number of tasks, core can be oversubscribed often with little perf degradation
</PRE>


<PRE>
parallel::mclapply - multi-core lapply - https://nceas.github.io/oss-lessons/parallel-computing-in-r/parallel-computing-in-r.html

instead of 
	result = lapply(starts, fx)
use
	result = mclapply(starts, fx, cores=4)
mclapply gathers up the responses from each of these function calls, and returns a list of responses that is the same length as the list or vector of input data (one return per input item).

starts is a variable holding data, eg rep(100, 40)
fx is a custom function that lapply() runs

pvec - for simple use case of applying func to each element of vector and return vector, pvec is easier to use
# https://ljdursi.github.io/beyond-single-core-R/#/48



https://ljdursi.github.io/beyond-single-core-R/#/30
mcparallel  # fork a task , run in background
mccollect   # wait and collect result.  so kinda like MPI scatter/gather


library(multicore)                    ## functionality replaced by library('parallel') 
fun1 = function() {Sys.sleep(10); 1}
fun2 = function() {Sys.sleep(5);  2}
f1 = parallel(fun1())                 ## new fn name is mcparallel()
f2 = parallel(fun2())
result = collect(list(f1, f2))

result[[1]]  # would contain result of f1

</PRE>

<!-- ## hmm... maybe parallelize SimAnneal this way??  ++ -->

Linear Regression Model fitting, in series vs parallel <BR>
https://ljdursi.github.io/beyond-single-core-R/#/32 and slide 33


<PRE>

fit1 =  lm(DISTANCE  ~ AIR_TIME,  data=jan2010))
fit2 =  lm(ARR_DELAY ~ DEP_DELAY, data=jan2010))

-vs-

parfits = function() {
     pfit1 = mcparallel(lm(DISTANCE  ~ AIR_TIME,  data=jan2010))
     pfit2 = mcparallel(lm(ARR_DELAY ~ DEP_DELAY, data=jan2010))
     mccollect( list(pfit1, pfit2) )
}
parfits()

Note that mcollect asked for the tasks output to be combined into a list.
Some data structure maybe needed to extract more complex model result??

</PRE>

<A HREF="https://blog.dominodatalab.com/multicore-data-science-r-python/">domino data lab</A> on caret/doMC 
<BR>

<PRE>

library(doMC)
library(caret) # popular ML using foreach to parallelize CV-folds, etc.
	fit.lda.ser vs
	fit.lda.par

</PRE>



<A NAME="cluster"></A>
<H2>Parallelization in R using cluster functions </H2>

makeCluster()  <BR>
stopCluster()  <BR>
doParallel()   <BR>
registerDoSEQ() <BR>

snow <BR>

foreach <BR>
%do% <BR>
%dopar% <BR>

<PRE>

for is run for its side effect
foreach returns result, side effects maybe gone (when using parallel backend).
foreach should be tought of lapply, not for loop


%do% is serial
%dopar% is parallel, and need a backend.
Parallel libs: doParallel, doMC, doMPI
  cl = parallel::makeCluster(4)
  cl = parallel::makeCluster(4, type = "FORK")   # unix fork() based, not usable in windows natively.
  cl = parallel::makeCluster(4, type = "PSOCK")  # default.  Parallel Sockets on a SNOW cluster with 4-node
  cl = parallel::makeForkCluster(4)

use registerDoSEQ() if want to force foreach to run in serial 

much of the parallelization is done using fork() in unix (may not work in windows).
Also, fork() allow access to vars/memory from parent, but result not returned at the end.
Need to do some special collection/return if need to save those data.
Thus, "side-effects" such as those produced by `for` vanishes in parallelization.

SNOW - like PSOCK, entirely new process, so ned to explicitly copy data.  but process can be on remote machine.


https://ljdursi.github.io/beyond-single-core-R/#/55
clusterCall()
clusterEvalQ() # easier to use for tasks


  cl = parallel::makeCluster(4, type = "PSOCK")  # Parallel Sockets creates a brand new session in each node, 
						 # R copies data and .packages to each node, so more overhead than fork().  
						 # 4 here indicate create a 4-node SNOW cluster, which are just cores in a SMP machine.   
						 # for multinode, provide a vector of hostnames, which R need to ssh to (and run 1 core per machine?) 
						 # https://stackoverflow.com/questions/36794063/r-foreach-from-single-machine-to-cluster
  multinode foreach call would need to have param for .packages to copy to each node, as well as .export for variables  eg:
  foreach(i = 1:nrow(data_frame_1), .packages = c("package_1","package_2"), .export = c("variable_1","variable_2"))  %dopar% { ... }     

If the foreach loop is in the global environment, variables should be exported automatically. If not, you can use .export = ls(globalenv()) (or .GlobalEnv).
per https://stackoverflow.com/questions/45767416/how-to-export-many-variables-and-functions-from-global-environment-to-foreach-lo



</PRE>

<A NAME="doParallel"></A>
<H3>doParallel</H3>

One way to do parallel processing in R

<PRE>
NumThread=15
library(doParallel)
cl = makeCluster( NumThread, outfile="mkcluster.out" )   ## create a cluster object
registerDoParallel(cl)
foreach(i=1:3) %dopar% sqrt(i)
# the %dopar% says to run the foreach loop in parallel for each item
# all console output inside cluster object will go to outfile 
# there are eg combine=rbind to merge result
stopCluster(cl)

.combine=rbind # row-bind, ie, each result becomes a row
.combine=cbind # col-bind, ie, each result becomes a col
.combine=c     # c() ?  concatenate?


detectCores() ## return number of cores on machine

</PRE>

Ref: 
<UL>
<LI><A HREF="https://cran.r-project.org/web/packages/doParallel/vignettes/gettingstartedParallel.pdf">CRAN doParallel</A></LI>

<LI><A HREF="https://www.rdocumentation.org/packages/doParallel/versions/1.0.15/topics/registerDoParallel">R Doc on doParallel</A></LI>
<LI><A HREF="https://nceas.github.io/oss-lessons/parallel-computing-in-r/parallel-computing-in-r.html"> Quick Intro to Parallel Computing in R</LI>
<LI><A HREF="https://ljdursi.github.io/beyond-single-core-R/#/">Beyond Single-Core R</A> (and github source at https://github.com/ljdursi/beyond-single-core-R</LI>
<LI></LI>
<LI></LI>
</UL>

<A NAME="Rdsm"></A>
<H3>Rdsm</H3>
https://ljdursi.github.io/beyond-single-core-R/#/139
<PRE>
library(parallel)
library(Rdsm)

nrows = 7
cl = makePSOCKcluster(3)       # form 3-process PSOCK (share-nothing) cluster
init = mgrinit(cl)             # initialize Rdsm
mgrmakevar(cl,"m",nrows,nrows)  # make a 7x7 shared matrix
bar = makebarr(cl)

clusterEvalQ(cl,myid = myinfo$id)

clusterExport(cl,c("nrows"))
dmy = clusterEvalQ(cl,myidxs = getidxs(nrows))
dmy = clusterEvalQ(cl, m[myidxs,1:nrows] = myid)
dmy = clusterEvalQ(cl,"barr()")

print(m[,])


</PRE>


<A NAME="Clustering_on_Clusters"></A>
<H3>Clustering on Clusters</H3>

https://ljdursi.github.io/beyond-single-core-R/#/57

<PRE>
ship data to other nodes of the cluster:
cl = makeCluster(8)
clusterExport(cl, "jan2010")
cares = clusterApply(cl, rep(5,4), do.n.kmeans)
stopCluster(cl)
mcres = mclapply(rep(5,4), do.n.kmeans, mc.cores=4)

</PRE>

https://ljdursi.github.io/beyond-single-core-R/#/59
<BR>

<PRE>
# Running across machines:
# 2 machine, each with 8 core of process?

hosts = c( rep("localhost",8), rep("192.168.0.10", 8) )
cl = makePSOCKcluster(names=hosts)
clusterCall(cl, rnorm, 5)
clusterCall(cl, system, "hostname")
stopCluster(cl)

</PRE>
<!-- ++ check for SimAnneal , but need to ship data to node ++ -->


<A NAME="snow"></A>
<H3>Snow</H3>

https://ljdursi.github.io/beyond-single-core-R/#/61
<BR>

<PRE>
scheduling
viz of run

library(snow,quiet=TRUE)

do.kmeans.nclusters <- function(n) { kmeans(jan2010, centers=n, nstart=10) }

cl = makeCluster(2)
clusterExport(cl,"jan2010")

tm = snow.time( clusterApply(cl, 1:6, do.kmeans.nclusters) )
plot(tm)

# load balance, below is more like mc.preschedule=FALSE, kicking off task to worker only when needed
tm.lb = snow.time(clusterApplyLB(cl, 1:6, do.kmeans.nclusters))
plot(tm.lb)

</PRE>

https://ljdursi.github.io/beyond-single-core-R/#/73 - Summary for parallel/snow

<LI> clusterExport good for for SIMD
<LI> clusterApplyLB good for MIMD
<LI> mcparallel is best for MIMD?
<LI> parLapply
<LI> makePSOCKcluster for small cluster
<LI> makeMPIcluster for larger cluster -- but read up on pbdR first
<LI> pdbR      -- https://ljdursi.github.io/beyond-single-core-R/#/143
<LI> pdb*apply -- https://ljdursi.github.io/beyond-single-core-R/#/152
<LI> using pdbR -- bring hpc distributed memory processing (SPMD) to R, see section 3.1.2 of <A HREF="https://htmlpreview.github.io/?https://github.com/berkeley-scf/tutorial-parallel-distributed/blob/master/parallel-dist.html">CPaciorek's Parallel Processing for Distributed Computing</A>
<LI>
<LI>

https://ljdursi.github.io/beyond-single-core-R/#/83

<PRE>
foreach parallel
.multicombine=TRUE

%:% nest foreach object # slide 84


</PRE>


MPI cluster 

<PRE>
https://ljdursi.github.io/beyond-single-core-R/#/149

init()
rank = comm.rank()
...
allreduce(length(dta), op="sum")
comm.print(data.min)

finalize()
</PRE>



<A NAME="Nested Parallel"></A>
<H3>nestedParallel</H3>

https://stackoverflow.com/questions/21306027/nesting-parallel-functions-in-r

<PRE>

cl = makeCluster(2, type = "SOCK")
clusterCall(cl, function() {
  library(doSNOW)
  registerDoSNOW(makeCluster(2, type = "SOCK"))
  NULL
})
registerDoSNOW(cl)

</PRE>


<BR>
<BR>

<H1>Parallel R Baggage</H1>


Some history of the R parallel back ends...
<BR>

see: https://stackoverflow.com/questions/28989855/the-difference-between-domc-and-doparallel-in-r

<PRE>
doParallel is essentially merge of multicore (doMC) and snow.

multicore is w/in same machine, so faster than snow.  
multicore is fork based.
snow was intended to leverage HPC MPI stack.   so registerDoSNOW() has more overhead than registerDoParallel().  

parallel is a merger of snow and multicore


registerDoParallel vs registerDoSNOW
registerDoParallel() could be invoking the multicore backend and use doMC/fork when on single node, thus less overhead than registerDoSNOW()

registerDoSEQ() is run in serial, useful for debugging to test whether parallel code produce same result as sequential code.

SNOW use PSOCK, easier to deal with than MPI.

</PRE>



<!--  ----------------------------------------------------------------------  -->
<!--  ----------------------------------------------------------------------  -->
<!--  ----------------------------------------------------------------------  -->
<!--  ----------------------------------------------------------------------  -->

<BR><HR>
<div align="CENTER">
  [Doc URL: <A HREF="http://tin6150.github.io/psg/R.html">
http://tin6150.github.io/psg/R.html</A>] <BR>
  [Doc URL: <A HREF="http://tin6150.gitlab.io/psg/R.html">
http://tin6150.gitlab.io/psg/R.html</A>] <BR>
Last Updated: 2022-01-08
<BR>
(cc) Tin Ho. See 
<A HREF=psg.html#cc>main page</A>
 for copyright info. <BR>
<HR>
<A HREF="http://www.taos.com"><IMG SRC="banner/taos_banner1.gif" width="728" height="98"></A>
</div>
<div class="sig"><BR>
  psg101 sn50 tin6150<BR>
  hoti1<BR>
  nSarCoV2<BR>
  bofh1</div>

</body>

<!-- Google analytics new tracking code ga.js.   Will actually need to add this code to every page for full tracking!    (still the case in 2011?) Using my gmail login 2011.0617 updated with code for http://tin6150.github.io/psg/psg.html -->    <script type="text/javascript">    var _gaq = _gaq || [];   _gaq.push(['_setAccount', 'UA-4515095-4']);   _gaq.push(['_trackPageview']);    (function() {     var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;     ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';     var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);   })();  </script>


<!-- vim: paste tabstop=8 
-->
</html>
